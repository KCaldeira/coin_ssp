#!/usr/bin/env python3
"""
COIN-SSP Post-Processing Analysis Framework

This script provides a comprehensive analysis framework for results generated by the main.py pipeline.
It takes as input the output directory created by main.py and runs a suite of post-processing analyses.

Usage:
    python main_postprocess.py /path/to/output_integrated_CanESM5_20250919_123456/

The script is designed to be modular, with each analysis function being self-contained and
independently runnable. Future versions may include flags to selectively run specific analyses.

Author: COIN-SSP Development Team
Created: September 2025
"""

import argparse
import os
import sys
import glob
import time
from datetime import datetime
from pathlib import Path

# Add current directory to path for imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def validate_output_directory(output_dir):
    """
    Validate that the provided directory contains expected COIN-SSP pipeline output files.

    Parameters
    ----------
    output_dir : str
        Path to the output directory from main.py pipeline

    Returns
    -------
    dict
        Dictionary containing paths to key files found in the directory

    Raises
    ------
    FileNotFoundError
        If required files are not found in the directory
    """
    output_path = Path(output_dir)

    if not output_path.exists():
        raise FileNotFoundError(f"Output directory does not exist: {output_dir}")

    if not output_path.is_dir():
        raise FileNotFoundError(f"Path is not a directory: {output_dir}")

    # Define expected file patterns
    expected_files = {
        'all_loaded_data': 'all_loaded_data_*.nc',
        'step1_target_gdp': 'step1_target_gdp_*.nc',
        'step2_baseline_tfp': 'step2_baseline_tfp_*.nc',
        'step3_scaling_factors': 'step3_scaling_factors_*.nc',
        'step4_forward_results': 'step4_forward_*.nc'  # Updated pattern for split files
    }

    found_files = {}
    missing_files = []

    for file_type, pattern in expected_files.items():
        matches = list(output_path.glob(pattern))
        if matches:
            found_files[file_type] = matches[0]  # Take first match
            print(f"  ‚úì Found {file_type}: {matches[0].name}")
        else:
            missing_files.append(f"{file_type} ({pattern})")

    if missing_files:
        print(f"  ‚ùå Missing required files: {', '.join(missing_files)}")
        raise FileNotFoundError(f"Missing required pipeline output files in {output_dir}")

    # Look for PDF visualizations (optional but informative)
    pdf_files = list(output_path.glob("*.pdf"))
    if pdf_files:
        print(f"  ‚úì Found {len(pdf_files)} PDF visualization files")
        found_files['pdf_files'] = pdf_files

    print(f"  ‚úì Directory validation successful: {len(found_files)} file types found")
    return found_files


def obtain_configuration_information(output_files, analysis_output_dir):
    """
    Extract configuration information from NetCDF files and return in main processing format.

    This function reads the JSON configuration that was embedded in the NetCDF files
    during pipeline processing and returns it in the same data structure format used
    by the main processing code, enabling reuse of main processing functions.

    Parameters
    ----------
    output_files : dict
        Dictionary containing paths to pipeline output files
    analysis_output_dir : str
        Directory where analysis results should be saved

    Returns
    -------
    dict
        Configuration dictionary in the same format as used by main processing code
    """
    import xarray as xr
    import json

    print("üîß Obtaining configuration information...")

    # Try to read configuration from NetCDF files (prefer all_loaded_data for completeness)
    config_data = None
    config_source = None

    for file_type in ['all_loaded_data', 'step1_target_gdp', 'step2_baseline_tfp', 'step3_scaling_factors', 'step4_forward_results']:
        if file_type in output_files:
            try:
                print(f"    Checking {file_type}...")
                ds = xr.open_dataset(output_files[file_type])

                # Look for configuration in global attributes (try both possible names)
                config_json = None
                if 'configuration_json' in ds.attrs:
                    config_json = ds.attrs['configuration_json']
                elif 'processing_config' in ds.attrs:
                    config_json = ds.attrs['processing_config']

                if config_json is not None:
                    config_data = json.loads(config_json)
                    config_source = file_type
                    ds.close()
                    print(f"    ‚úì Configuration found in {file_type}")
                    break

                ds.close()

            except Exception as e:
                print(f"    ‚ùå Error reading {file_type}: {e}")
                continue

    if config_data is None:
        raise ValueError("No configuration data found in any NetCDF file")

    # Validate that we have the expected main processing format
    expected_sections = ['climate_model', 'ssp_scenarios', 'time_periods', 'model_params']
    missing_sections = [section for section in expected_sections if section not in config_data]

    if missing_sections:
        print(f"    ‚ö†Ô∏è  Warning: Missing expected configuration sections: {missing_sections}")

    # Log key configuration information
    print(f"    üìÑ Configuration extracted from: {config_source}")

    if 'run_metadata' in config_data:
        run_meta = config_data['run_metadata']
        print(f"    üè∑Ô∏è  Run: {run_meta.get('run_name', 'Unknown')}")

    if 'climate_model' in config_data:
        print(f"    üå°Ô∏è  Model: {config_data['climate_model'].get('model_name', 'Unknown')}")

    if 'ssp_scenarios' in config_data:
        ssp_info = config_data['ssp_scenarios']
        print(f"    üåç Reference SSP: {ssp_info.get('reference_ssp', 'Unknown')}")

    print(f"    ‚úÖ Configuration ready for reuse with main processing functions")

    return config_data


def analysis_temperature_gdp_correlations(output_files, analysis_output_dir, config):
    """
    Analyze correlations between LOESS-smoothed temperature and GDP variables.

    For each SSP/target/response function combination, computes correlations between
    30-year LOESS smoothed temp_ssp and GDP variables (gdp_weather, gdp_climate).
    Reports correlation coefficient, linear regression slope, and ratio of standard deviations.

    Parameters
    ----------
    output_files : dict
        Dictionary containing paths to pipeline output files
    analysis_output_dir : str
        Directory where analysis results should be saved
    config : dict
        Configuration dictionary in main processing format
    """
    import xarray as xr
    import numpy as np
    import pandas as pd
    import glob
    import os
    from scipy import stats

    # Import LOESS function from main processing code
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from coin_ssp_utils import apply_time_series_filter

    print("üìä Analysis: Temperature-GDP Correlations with LOESS Smoothing")

    # Create analysis output directory
    analysis_dir = os.path.join(analysis_output_dir, "temperature_gdp_correlations")
    os.makedirs(analysis_dir, exist_ok=True)
    print(f"    Output directory: {analysis_dir}")

    # Load temperature data from all_loaded_data file
    print("    üìà Loading temperature data...")
    temp_ds = xr.open_dataset(output_files['all_loaded_data'])
    temperature_data = temp_ds.temperature.values  # [ssp, time, lat, lon]
    temp_coords = {
        'ssp': temp_ds.ssp.values,
        'time': temp_ds.time.values,
        'lat': temp_ds.lat.values,
        'lon': temp_ds.lon.values
    }
    valid_mask = temp_ds.valid_mask.values  # [lat, lon]
    temp_ds.close()

    print(f"    Temperature data shape: {temperature_data.shape}")
    print(f"    Valid grid cells: {np.sum(valid_mask)}")

    # Find all step4 GDP NetCDF files
    output_dir_path = os.path.dirname(list(output_files.values())[0])
    step4_pattern = os.path.join(output_dir_path, "step4_forward_*_gdp_*.nc")
    step4_files = glob.glob(step4_pattern)

    if not step4_files:
        raise FileNotFoundError("No step4 GDP files found")

    print(f"    üìÅ Found {len(step4_files)} step4 GDP files")

    # Use LOESS filtering over the entire simulation period
    time_periods = config['time_periods']

    # Get full time range for LOESS filtering
    full_start_idx = 0
    full_end_idx = len(temp_coords['time']) - 1

    print(f"    üéØ LOESS filtering over full time series: {temp_coords['time'][0]}-{temp_coords['time'][-1]} (indices {full_start_idx}-{full_end_idx})")

    # Initialize results storage
    correlation_results = []

    # Process each step4 file
    for step4_file in step4_files:
        print(f"\n    üìä Processing: {os.path.basename(step4_file)}")

        # Load step4 data
        step4_ds = xr.open_dataset(step4_file)

        # Extract SSP name and variable type from filename or dataset attributes
        ssp_name = step4_ds.attrs.get('ssp_scenario', 'unknown')
        var_type = step4_ds.attrs.get('variable_type', 'unknown')

        # Extract GDP data arrays
        gdp_climate = step4_ds.gdp_climate.values  # [target, response_func, time, lat, lon]
        gdp_weather = step4_ds.gdp_weather.values  # [target, response_func, time, lat, lon]

        # Get coordinate information
        target_names = step4_ds.target.values
        response_func_names = step4_ds.response_func.values
        step4_time = step4_ds.time.values

        step4_ds.close()

        # Find corresponding temperature data for this SSP
        ssp_idx = np.where(temp_coords['ssp'] == ssp_name)[0]
        if len(ssp_idx) == 0:
            print(f"      ‚ùå No temperature data found for SSP {ssp_name}")
            continue

        ssp_idx = ssp_idx[0]
        temp_ssp = temperature_data[ssp_idx]  # [time, lat, lon]

        print(f"      üìã SSP: {ssp_name}, Variable: GDP")
        print(f"      üìã Targets: {len(target_names)}, Response functions: {len(response_func_names)}")

        # Process each target/response_func combination
        for target_idx, target_name in enumerate(target_names):
            for resp_idx, resp_func_name in enumerate(response_func_names):

                # Extract GDP data for this combination
                gdp_climate_combo = gdp_climate[target_idx, resp_idx]  # [time, lat, lon]
                gdp_weather_combo = gdp_weather[target_idx, resp_idx]  # [time, lat, lon]

                # Process each valid grid cell
                valid_cell_count = 0
                correlations_climate = []
                correlations_weather = []
                slopes_climate = []
                slopes_weather = []
                std_ratios_climate = []
                std_ratios_weather = []

                for lat_idx in range(temp_ssp.shape[1]):
                    for lon_idx in range(temp_ssp.shape[2]):
                        if not valid_mask[lat_idx, lon_idx]:
                            continue

                        # Extract time series for this grid cell
                        temp_series = temp_ssp[:, lat_idx, lon_idx]
                        gdp_climate_series = gdp_climate_combo[:, lat_idx, lon_idx]
                        gdp_weather_series = gdp_weather_combo[:, lat_idx, lon_idx]

                        # Skip if any data is invalid
                        if (np.any(~np.isfinite(temp_series)) or
                            np.any(~np.isfinite(gdp_climate_series)) or
                            np.any(~np.isfinite(gdp_weather_series))):
                            continue

                        # Apply 30-year LOESS smoothing over entire time series
                        try:
                            temp_smoothed = apply_time_series_filter(temp_series, 30, full_start_idx, full_end_idx)
                            gdp_climate_smoothed = apply_time_series_filter(gdp_climate_series, 30, full_start_idx, full_end_idx)
                            gdp_weather_smoothed = apply_time_series_filter(gdp_weather_series, 30, full_start_idx, full_end_idx)
                        except Exception as e:
                            continue

                        # Calculate correlations and regression statistics
                        # Climate scenario (temp_ssp vs gdp_climate)
                        corr_climate, _ = stats.pearsonr(temp_smoothed, gdp_climate_smoothed)
                        slope_climate, _, _, _, _ = stats.linregress(temp_smoothed, gdp_climate_smoothed)
                        std_ratio_climate = np.std(gdp_climate_smoothed) / np.std(temp_smoothed)

                        # Weather scenario (temp_ssp vs gdp_weather)
                        corr_weather, _ = stats.pearsonr(temp_smoothed, gdp_weather_smoothed)
                        slope_weather, _, _, _, _ = stats.linregress(temp_smoothed, gdp_weather_smoothed)
                        std_ratio_weather = np.std(gdp_weather_smoothed) / np.std(temp_smoothed)

                        # Store results
                        correlations_climate.append(corr_climate)
                        correlations_weather.append(corr_weather)
                        slopes_climate.append(slope_climate)
                        slopes_weather.append(slope_weather)
                        std_ratios_climate.append(std_ratio_climate)
                        std_ratios_weather.append(std_ratio_weather)

                        valid_cell_count += 1

                # Calculate summary statistics for this target/response combination
                if valid_cell_count > 0:
                    result = {
                        'ssp': ssp_name,
                        'variable': 'gdp',
                        'target': target_name,
                        'response_function': resp_func_name,
                        'valid_cells': valid_cell_count,
                        'corr_climate_mean': np.mean(correlations_climate),
                        'corr_climate_median': np.median(correlations_climate),
                        'corr_climate_std': np.std(correlations_climate),
                        'corr_weather_mean': np.mean(correlations_weather),
                        'corr_weather_median': np.median(correlations_weather),
                        'corr_weather_std': np.std(correlations_weather),
                        'slope_climate_mean': np.mean(slopes_climate),
                        'slope_climate_median': np.median(slopes_climate),
                        'slope_climate_std': np.std(slopes_climate),
                        'slope_weather_mean': np.mean(slopes_weather),
                        'slope_weather_median': np.median(slopes_weather),
                        'slope_weather_std': np.std(slopes_weather),
                        'std_ratio_climate_mean': np.mean(std_ratios_climate),
                        'std_ratio_climate_median': np.median(std_ratios_climate),
                        'std_ratio_climate_std': np.std(std_ratios_climate),
                        'std_ratio_weather_mean': np.mean(std_ratios_weather),
                        'std_ratio_weather_median': np.median(std_ratios_weather),
                        'std_ratio_weather_std': np.std(std_ratios_weather)
                    }

                    correlation_results.append(result)

                    print(f"        ‚úì {target_name} √ó {resp_func_name}: {valid_cell_count} cells, "
                          f"r_climate={result['corr_climate_mean']:.3f} (med={result['corr_climate_median']:.3f}), "
                          f"r_weather={result['corr_weather_mean']:.3f} (med={result['corr_weather_median']:.3f})")

    # Create results DataFrame and save to CSV
    if correlation_results:
        df = pd.DataFrame(correlation_results)
        csv_path = os.path.join(analysis_dir, "climate_economic_correlations.csv")
        df.to_csv(csv_path, index=False)

        print(f"\n    üíæ Results saved to: climate_economic_correlations.csv")
        print(f"    üìä Total combinations analyzed: {len(correlation_results)}")

        # Display summary to terminal
        print(f"\n    üìã CORRELATION ANALYSIS SUMMARY:")
        print(f"    {'=' * 60}")

        for _, row in df.iterrows():
            print(f"    {row['ssp'].upper()} √ó {row['variable'].upper()} √ó {row['target']} √ó {row['response_function']}")
            print(f"      Valid cells: {row['valid_cells']}")
            print(f"      Climate correlation: {row['corr_climate_mean']:.3f} ¬± {row['corr_climate_std']:.3f}")
            print(f"      Weather correlation: {row['corr_weather_mean']:.3f} ¬± {row['corr_weather_std']:.3f}")
            print(f"      Climate slope: {row['slope_climate_mean']:.3f} ¬± {row['slope_climate_std']:.3f}")
            print(f"      Weather slope: {row['slope_weather_mean']:.3f} ¬± {row['slope_weather_std']:.3f}")
            print(f"      Climate std ratio: {row['std_ratio_climate_mean']:.3f} ¬± {row['std_ratio_climate_std']:.3f}")
            print(f"      Weather std ratio: {row['std_ratio_weather_mean']:.3f} ¬± {row['std_ratio_weather_std']:.3f}")
            print()

        print(f"    ‚úÖ Climate-economic correlation analysis completed")
    else:
        print(f"    ‚ùå No valid results generated")
        raise ValueError("No correlation results were generated")


def analysis_3_placeholder(output_files, analysis_output_dir, config):
    """
    Placeholder for third post-processing analysis.

    Parameters
    ----------
    output_files : dict
        Dictionary containing paths to pipeline output files
    analysis_output_dir : str
        Directory where analysis results should be saved
    config : dict
        Configuration dictionary in main processing format (from obtain_configuration_information)
    """
    print("üìä Analysis 3: [Future analysis placeholder]")
    print("    This analysis slot is ready for future implementation...")
    print(f"    Configuration available: {config['climate_model']['model_name']} model")


def run_all_analyses(output_files, analysis_output_dir):
    """
    Execute all post-processing analyses in sequence.

    First obtains configuration information to enable reuse of main processing functions,
    then runs all specific analyses with access to both output files and configuration.

    Parameters
    ----------
    output_files : dict
        Dictionary containing paths to pipeline output files
    analysis_output_dir : str
        Directory where analysis results should be saved

    Returns
    -------
    dict
        Dictionary containing analysis execution results and timing
    """
    analysis_results = {}
    total_start_time = time.time()

    print(f"\nüî¨ Starting post-processing analysis framework...")
    print("=" * 70)

    # First, obtain configuration information (required for all subsequent analyses)
    print(f"\nüîç Step 1: Obtaining configuration information...")
    config_start = time.time()

    try:
        config = obtain_configuration_information(output_files, analysis_output_dir)
        config_time = time.time() - config_start
        analysis_results['Configuration'] = {
            'status': 'completed',
            'duration': config_time
        }
        print(f"    ‚úÖ Configuration obtained in {config_time:.1f}s")

    except Exception as e:
        config_time = time.time() - config_start
        analysis_results['Configuration'] = {
            'status': 'failed',
            'duration': config_time,
            'error': str(e)
        }
        print(f"    ‚ùå Configuration failed after {config_time:.1f}s: {e}")
        print(f"    ‚ö†Ô∏è  Cannot proceed with analyses without configuration data")

        total_time = time.time() - total_start_time
        analysis_results['total_duration'] = total_time
        return analysis_results

    # Define subsequent analyses (config is now available)
    analyses = [
        ("Temperature-GDP Correlations", analysis_temperature_gdp_correlations),
        ("Analysis 3", analysis_3_placeholder)
    ]

    # Run remaining analyses with both output_files and config available
    print(f"\nüî¨ Running {len(analyses)} post-processing analyses...")

    for analysis_name, analysis_func in analyses:
        print(f"\nüîç Starting {analysis_name}...")
        analysis_start = time.time()

        try:
            # Pass both output_files and config to analyses
            analysis_func(output_files, analysis_output_dir, config)
            analysis_time = time.time() - analysis_start
            analysis_results[analysis_name] = {
                'status': 'completed',
                'duration': analysis_time
            }
            print(f"    ‚úÖ {analysis_name} completed in {analysis_time:.1f}s")

        except Exception as e:
            analysis_time = time.time() - analysis_start
            analysis_results[analysis_name] = {
                'status': 'failed',
                'duration': analysis_time,
                'error': str(e)
            }
            print(f"    ‚ùå {analysis_name} failed after {analysis_time:.1f}s: {e}")

    total_time = time.time() - total_start_time
    analysis_results['total_duration'] = total_time

    print("=" * 70)
    print(f"üìã Post-processing Summary:")

    successful = sum(1 for r in analysis_results.values()
                    if isinstance(r, dict) and r.get('status') == 'completed')
    failed = sum(1 for r in analysis_results.values()
                if isinstance(r, dict) and r.get('status') == 'failed')

    print(f"    ‚úÖ Successful analyses: {successful}")
    print(f"    ‚ùå Failed analyses: {failed}")
    print(f"    ‚è±Ô∏è  Total processing time: {total_time:.1f}s")

    return analysis_results


def setup_analysis_output_directory(pipeline_output_dir):
    """
    Create a dedicated directory for post-processing analysis results.

    Parameters
    ----------
    pipeline_output_dir : str
        Path to the pipeline output directory

    Returns
    -------
    str
        Path to the created analysis output directory
    """
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

    # Create analysis directory alongside pipeline output
    pipeline_parent = os.path.dirname(os.path.abspath(pipeline_output_dir))
    pipeline_dirname = os.path.basename(os.path.abspath(pipeline_output_dir))

    # Create analysis directory name based on pipeline directory
    analysis_dirname = f"postprocess_{pipeline_dirname}_{timestamp}"
    analysis_output_dir = os.path.join(pipeline_parent, analysis_dirname)

    os.makedirs(analysis_output_dir, exist_ok=True)

    print(f"üìÅ Analysis output directory: {analysis_output_dir}")
    return analysis_output_dir


def main():
    """Main post-processing entry point."""
    parser = argparse.ArgumentParser(
        description="COIN-SSP Post-Processing Analysis Framework",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python main_postprocess.py data/output/output_integrated_CanESM5_20250919_123456/
  python main_postprocess.py /path/to/pipeline/output/directory/

This framework analyzes results from the main.py COIN-SSP processing pipeline.
All analyses are run by default. Future versions may include selective analysis flags.
        """
    )

    parser.add_argument(
        'output_directory',
        help='Path to the output directory created by main.py pipeline'
    )

    args = parser.parse_args()

    print("üåç COIN-SSP Post-Processing Analysis Framework")
    print("=" * 60)
    print(f"Pipeline output directory: {args.output_directory}")
    print(f"Analysis started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    try:
        # Validate input directory and find pipeline output files
        print(f"\nüîç Validating pipeline output directory...")
        output_files = validate_output_directory(args.output_directory)

        # Setup analysis output directory
        analysis_output_dir = setup_analysis_output_directory(args.output_directory)

        # Run all post-processing analyses
        analysis_results = run_all_analyses(output_files, analysis_output_dir)

        print(f"\nüéâ Post-processing analysis completed!")
        print(f"üìÅ Results saved to: {analysis_output_dir}")

        return 0

    except Exception as e:
        print(f"\n‚ùå Post-processing failed: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())