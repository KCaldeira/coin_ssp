#!/usr/bin/env python3
"""
COIN-SSP Post-Processing Analysis Framework

This script provides a comprehensive analysis framework for results generated by the main.py pipeline.
It takes as input the output directory created by main.py and runs a suite of post-processing analyses.

Usage:
    python main_postprocess.py /path/to/output_integrated_CanESM5_20250919_123456/

The script is designed to be modular, with each analysis function being self-contained and
independently runnable. Future versions may include flags to selectively run specific analyses.

Author: COIN-SSP Development Team
Created: September 2025
"""

import argparse
import os
import sys
import glob
import time
from datetime import datetime
from pathlib import Path
import numpy as np
import pandas as pd
import xarray as xr
import statsmodels.api as sm
from scipy import stats

# Add current directory to path for imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def format_number(value):
    """
    Format number with appropriate precision:
    - Show 1.23e-03 for very small numbers (< 0.01)
    - Show 0.1234 for numbers between 0.01 and 1
    - Show 1.234 for numbers ≥ 1

    Parameters
    ----------
    value : float
        Number to format

    Returns
    -------
    str
        Formatted number string
    """
    if np.isnan(value) or np.isinf(value):
        return str(value)

    abs_value = abs(value)
    if abs_value == 0:
        return "0.0000"
    elif abs_value < 0.01:
        # Use scientific notation for small numbers (1.23e-03)
        return f"{value:.2e}"
    elif abs_value < 1:
        # Show 4 decimal places for numbers between 0.01 and 1 (0.1234)
        return f"{value:.4f}"
    else:
        # Show 3 decimal places for numbers ≥ 1 (1.234)
        return f"{value:.3f}"

def validate_output_directory(output_dir):
    """
    Validate that the provided directory contains expected COIN-SSP pipeline output files.

    Parameters
    ----------
    output_dir : str
        Path to the output directory from main.py pipeline

    Returns
    -------
    dict
        Dictionary containing paths to key files found in the directory

    Raises
    ------
    FileNotFoundError
        If required files are not found in the directory
    """
    output_path = Path(output_dir)

    if not output_path.exists():
        raise FileNotFoundError(f"Output directory does not exist: {output_dir}")

    if not output_path.is_dir():
        raise FileNotFoundError(f"Path is not a directory: {output_dir}")

    # Define expected file patterns
    expected_files = {
        'all_loaded_data': 'all_loaded_data_*.nc',
        'step1_target_gdp': 'step1_target_gdp_*.nc',
        'step2_baseline_tfp': 'step2_baseline_tfp_*.nc',
        'step3_scaling_factors': 'step3_scaling_factors_*.nc',
        'step4_forward_results': 'step4_forward_*.nc'  # Updated pattern for split files
    }

    found_files = {}
    missing_files = []

    for file_type, pattern in expected_files.items():
        matches = list(output_path.glob(pattern))
        if matches:
            found_files[file_type] = matches[0]  # Take first match
            print(f"  ✓ Found {file_type}: {matches[0].name}")
        else:
            missing_files.append(f"{file_type} ({pattern})")

    if missing_files:
        print(f"  ❌ Missing required files: {', '.join(missing_files)}")
        raise FileNotFoundError(f"Missing required pipeline output files in {output_dir}")

    # Look for PDF visualizations (optional but informative)
    pdf_files = list(output_path.glob("*.pdf"))
    if pdf_files:
        print(f"  ✓ Found {len(pdf_files)} PDF visualization files")
        found_files['pdf_files'] = pdf_files

    print(f"  ✓ Directory validation successful: {len(found_files)} file types found")
    return found_files


def obtain_configuration_information(output_files, analysis_output_dir):
    """
    Extract configuration information from NetCDF files and return in main processing format.

    This function reads the JSON configuration that was embedded in the NetCDF files
    during pipeline processing and returns it in the same data structure format used
    by the main processing code, enabling reuse of main processing functions.

    Parameters
    ----------
    output_files : dict
        Dictionary containing paths to pipeline output files
    analysis_output_dir : str
        Directory where analysis results should be saved

    Returns
    -------
    dict
        Configuration dictionary in the same format as used by main processing code
    """
    import xarray as xr
    import json

    print("🔧 Obtaining configuration information...")

    # Try to read configuration from NetCDF files (prefer all_loaded_data for completeness)
    config_data = None
    config_source = None

    for file_type in ['all_loaded_data', 'step1_target_gdp', 'step2_baseline_tfp', 'step3_scaling_factors', 'step4_forward_results']:
        if file_type in output_files:
            try:
                print(f"    Checking {file_type}...")
                ds = xr.open_dataset(output_files[file_type])

                # Look for configuration in global attributes (try both possible names)
                config_json = None
                if 'configuration_json' in ds.attrs:
                    config_json = ds.attrs['configuration_json']
                elif 'processing_config' in ds.attrs:
                    config_json = ds.attrs['processing_config']

                if config_json is not None:
                    config_data = json.loads(config_json)
                    config_source = file_type
                    ds.close()
                    print(f"    ✓ Configuration found in {file_type}")
                    break

                ds.close()

            except Exception as e:
                print(f"    ❌ Error reading {file_type}: {e}")
                continue

    if config_data is None:
        raise ValueError("No configuration data found in any NetCDF file")

    # Validate that we have the expected main processing format
    expected_sections = ['climate_model', 'ssp_scenarios', 'time_periods', 'model_params']
    missing_sections = [section for section in expected_sections if section not in config_data]

    if missing_sections:
        print(f"    ⚠️  Warning: Missing expected configuration sections: {missing_sections}")

    # Log key configuration information
    print(f"    📄 Configuration extracted from: {config_source}")

    if 'run_metadata' in config_data:
        run_meta = config_data['run_metadata']
        print(f"    🏷️  Run: {run_meta.get('run_name', 'Unknown')}")

    if 'climate_model' in config_data:
        print(f"    🌡️  Model: {config_data['climate_model'].get('model_name', 'Unknown')}")

    if 'ssp_scenarios' in config_data:
        ssp_info = config_data['ssp_scenarios']
        print(f"    🌍 Reference SSP: {ssp_info.get('reference_ssp', 'Unknown')}")

    print(f"    ✅ Configuration ready for reuse with main processing functions")

    return config_data


def analysis_temperature_gdp_correlations(output_files, analysis_output_dir, config, ssp_filter=None):
    """
    Analyze correlations between temperature and GDP variability (detrended residuals).

    For each SSP/target/response function combination, computes correlations between
    temperature variability and GDP variability after removing 30-year LOESS trends.
    This shows how short-term climate fluctuations relate to short-term economic fluctuations.
    Reports correlation coefficient, linear regression slope, and ratio of standard deviations.

    Parameters
    ----------
    output_files : dict
        Dictionary containing paths to pipeline output files
    analysis_output_dir : str
        Directory where analysis results should be saved
    config : dict
        Configuration dictionary in main processing format
    """
    # Import LOESS function from main processing code (no longer needed since we use statsmodels directly)
    # from coin_ssp_utils import apply_time_series_filter

    print("📊 Analysis: Temperature-GDP Variability Correlations (Detrended)")

    # Create analysis output directory
    analysis_dir = os.path.join(analysis_output_dir, "temperature_gdp_variability_correlations")
    os.makedirs(analysis_dir, exist_ok=True)
    print(f"    Output directory: {analysis_dir}")

    # Load temperature data from all_loaded_data file
    print("    📈 Loading temperature data...")
    temp_ds = xr.open_dataset(output_files['all_loaded_data'])
    temperature_data = temp_ds.temperature.values  # [ssp, time, lat, lon]
    temp_coords = {
        'ssp': temp_ds.ssp.values,
        'time': temp_ds.time.values,
        'lat': temp_ds.lat.values,
        'lon': temp_ds.lon.values
    }
    valid_mask = temp_ds.valid_mask.values  # [lat, lon]
    temp_ds.close()

    print(f"    Temperature data shape: {temperature_data.shape}")
    print(f"    Valid grid cells: {np.sum(valid_mask)}")

    # Find all step4 GDP NetCDF files
    output_dir_path = os.path.dirname(list(output_files.values())[0])
    step4_pattern = os.path.join(output_dir_path, "step4_forward_*_gdp_*.nc")
    step4_files = glob.glob(step4_pattern)

    if not step4_files:
        raise FileNotFoundError("No step4 GDP files found")

    # Filter step4 files by SSP if specified
    if ssp_filter:
        filtered_files = []
        for filepath in step4_files:
            filename = os.path.basename(filepath)
            # Extract SSP from filename pattern: step4_forward_ssp126_gdp_CanESM5.nc
            if any(ssp in filename for ssp in ssp_filter):
                filtered_files.append(filepath)
        step4_files = filtered_files
        print(f"    🔍 Filtered to {len(step4_files)} step4 GDP files for SSPs: {', '.join(ssp_filter)}")
    else:
        print(f"    📁 Found {len(step4_files)} step4 GDP files")

    if not step4_files:
        if ssp_filter:
            raise FileNotFoundError(f"No step4 GDP files found for SSPs: {', '.join(ssp_filter)}")
        else:
            raise FileNotFoundError("No step4 GDP files found")

    # Use LOESS filtering over the entire simulation period
    time_periods = config['time_periods']

    # Get full time range for LOESS filtering
    full_start_idx = 0
    full_end_idx = len(temp_coords['time']) - 1

    print(f"    🎯 LOESS detrending over full time series: {temp_coords['time'][0]}-{temp_coords['time'][-1]} (indices {full_start_idx}-{full_end_idx})")

    # Initialize results storage
    correlation_results = []

    # Process each step4 file
    for step4_file in step4_files:
        print(f"\n    📊 Processing: {os.path.basename(step4_file)}")

        # Load step4 data
        step4_ds = xr.open_dataset(step4_file)

        # Extract SSP name and variable type from filename or dataset attributes
        ssp_name = step4_ds.attrs.get('ssp_scenario', 'unknown')
        var_type = step4_ds.attrs.get('variable_type', 'unknown')

        # Extract GDP data arrays
        gdp_climate = step4_ds.gdp_climate.values  # [target, response_func, time, lat, lon]
        gdp_weather = step4_ds.gdp_weather.values  # [target, response_func, time, lat, lon]

        # Get coordinate information
        target_names = step4_ds.target.values
        response_func_names = step4_ds.response_func.values
        step4_time = step4_ds.time.values

        step4_ds.close()

        # Find corresponding temperature data for this SSP
        ssp_idx = np.where(temp_coords['ssp'] == ssp_name)[0]
        if len(ssp_idx) == 0:
            print(f"      ❌ No temperature data found for SSP {ssp_name}")
            continue

        ssp_idx = ssp_idx[0]
        temp_ssp = temperature_data[ssp_idx]  # [time, lat, lon]

        print(f"      📋 SSP: {ssp_name}, Variable: GDP")
        print(f"      📋 Targets: {len(target_names)}, Response functions: {len(response_func_names)}")

        # Process each target/response_func combination
        # TODO: Add command line arguments to limit which SSP cases and climate damage targets are processed
        for target_idx, target_name in enumerate(target_names):
            for resp_idx, resp_func_name in enumerate(response_func_names):

                # Extract GDP data for this combination
                gdp_climate_combo = gdp_climate[target_idx, resp_idx]  # [time, lat, lon]
                gdp_weather_combo = gdp_weather[target_idx, resp_idx]  # [time, lat, lon]

                # Process each valid grid cell
                valid_cell_count = 0
                correlations_climate = []
                correlations_weather = []
                slopes_climate = []
                slopes_weather = []
                std_ratios_climate = []
                std_ratios_weather = []

                for lat_idx in range(temp_ssp.shape[1]):
                    for lon_idx in range(temp_ssp.shape[2]):
                        if not valid_mask[lat_idx, lon_idx]:
                            continue

                        # Extract time series for this grid cell
                        temp_series = temp_ssp[:, lat_idx, lon_idx]
                        gdp_climate_series = gdp_climate_combo[:, lat_idx, lon_idx]
                        gdp_weather_series = gdp_weather_combo[:, lat_idx, lon_idx]

                        # Skip if any data is invalid
                        if (np.any(~np.isfinite(temp_series)) or
                            np.any(~np.isfinite(gdp_climate_series)) or
                            np.any(~np.isfinite(gdp_weather_series))):
                            continue

                        # Apply 30-year LOESS smoothing over entire time series
                        # Note: apply_time_series_filter returns detrended data, we need the trend itself

                        # Get LOESS trends directly using lowess
                        t = np.arange(len(temp_series), dtype=float)
                        frac = min(1.0, 30 / len(temp_series))

                        temp_trend = sm.nonparametric.lowess(temp_series, t, frac=frac, it=1, return_sorted=False)
                        gdp_climate_trend = sm.nonparametric.lowess(gdp_climate_series, t, frac=frac, it=1, return_sorted=False)
                        gdp_weather_trend = sm.nonparametric.lowess(gdp_weather_series, t, frac=frac, it=1, return_sorted=False)

                        # Calculate variability
                        # for temperature: subtract smoothed trend (absolute deviations)
                        temp_variability = temp_series - temp_trend
                        # for GDP: divide by smoothed trend to get relative variability (fractional changes)
                        gdp_climate_variability = gdp_climate_series / gdp_climate_trend
                        gdp_weather_variability = gdp_weather_series / gdp_weather_trend


                        # Calculate correlations and regression statistics
                        # Climate scenario (temp_ssp vs gdp_climate)
                        corr_climate, _ = stats.pearsonr(temp_variability, gdp_climate_variability)
                        slope_climate, _, _, _, _ = stats.linregress(temp_variability, gdp_climate_variability)
                        std_ratio_climate = np.std(gdp_climate_variability) / np.std(temp_variability)

                        # Weather scenario (temp_ssp vs gdp_weather)
                        corr_weather, _ = stats.pearsonr(temp_variability, gdp_weather_variability)
                        slope_weather, _, _, _, _ = stats.linregress(temp_variability, gdp_weather_variability)
                        std_ratio_weather = np.std(gdp_weather_variability) / np.std(temp_variability)

                        # Store results
                        correlations_climate.append(corr_climate)
                        correlations_weather.append(corr_weather)
                        slopes_climate.append(slope_climate)
                        slopes_weather.append(slope_weather)
                        std_ratios_climate.append(std_ratio_climate)
                        std_ratios_weather.append(std_ratio_weather)

                        valid_cell_count += 1

                # Calculate summary statistics for this target/response combination
                if valid_cell_count > 0:
                    result = {
                        'ssp': ssp_name,
                        'variable': 'gdp',
                        'target': target_name,
                        'response_function': resp_func_name,
                        'valid_cells': valid_cell_count,
                        'corr_climate_mean': np.mean(correlations_climate),
                        'corr_climate_median': np.median(correlations_climate),
                        'corr_climate_std': np.std(correlations_climate),
                        'corr_weather_mean': np.mean(correlations_weather),
                        'corr_weather_median': np.median(correlations_weather),
                        'corr_weather_std': np.std(correlations_weather),
                        'slope_climate_mean': np.mean(slopes_climate),
                        'slope_climate_median': np.median(slopes_climate),
                        'slope_climate_std': np.std(slopes_climate),
                        'slope_weather_mean': np.mean(slopes_weather),
                        'slope_weather_median': np.median(slopes_weather),
                        'slope_weather_std': np.std(slopes_weather),
                        'std_ratio_climate_mean': np.mean(std_ratios_climate),
                        'std_ratio_climate_median': np.median(std_ratios_climate),
                        'std_ratio_climate_std': np.std(std_ratios_climate),
                        'std_ratio_weather_mean': np.mean(std_ratios_weather),
                        'std_ratio_weather_median': np.median(std_ratios_weather),
                        'std_ratio_weather_std': np.std(std_ratios_weather)
                    }

                    correlation_results.append(result)

                    print(f"        ✓ {target_name} × {resp_func_name}: {valid_cell_count} cells")
                    print(f"          r_climate: {format_number(result['corr_climate_mean'])} (med={format_number(result['corr_climate_median'])}, std={format_number(result['corr_climate_std'])})")
                    print(f"          r_weather: {format_number(result['corr_weather_mean'])} (med={format_number(result['corr_weather_median'])}, std={format_number(result['corr_weather_std'])})")
                    print(f"          slope_climate: {format_number(result['slope_climate_mean'])} (med={format_number(result['slope_climate_median'])}, std={format_number(result['slope_climate_std'])})")
                    print(f"          slope_weather: {format_number(result['slope_weather_mean'])} (med={format_number(result['slope_weather_median'])}, std={format_number(result['slope_weather_std'])})")
                    print(f"          stdratio_climate: {format_number(result['std_ratio_climate_mean'])} (med={format_number(result['std_ratio_climate_median'])}, std={format_number(result['std_ratio_climate_std'])})")
                    print(f"          stdratio_weather: {format_number(result['std_ratio_weather_mean'])} (med={format_number(result['std_ratio_weather_median'])}, std={format_number(result['std_ratio_weather_std'])})")

    # Create results DataFrame and save to CSV
    if correlation_results:
        df = pd.DataFrame(correlation_results)
        csv_path = os.path.join(analysis_dir, "climate_economic_correlations.csv")
        df.to_csv(csv_path, index=False)

        print(f"\n    💾 Results saved to: climate_economic_correlations.csv")
        print(f"    📊 Total combinations analyzed: {len(correlation_results)}")

        # Display summary to terminal
        print(f"\n    📋 CORRELATION ANALYSIS SUMMARY:")
        print(f"    {'=' * 60}")

        for _, row in df.iterrows():
            print(f"    {row['ssp'].upper()} × {row['variable'].upper()} × {row['target']} × {row['response_function']}")
            print(f"      Valid cells: {row['valid_cells']}")
            print(f"      Climate correlation: {row['corr_climate_mean']:.3f} ± {row['corr_climate_std']:.3f}")
            print(f"      Weather correlation: {row['corr_weather_mean']:.3f} ± {row['corr_weather_std']:.3f}")
            print(f"      Climate slope: {row['slope_climate_mean']:.3f} ± {row['slope_climate_std']:.3f}")
            print(f"      Weather slope: {row['slope_weather_mean']:.3f} ± {row['slope_weather_std']:.3f}")
            print(f"      Climate std ratio: {row['std_ratio_climate_mean']:.3f} ± {row['std_ratio_climate_std']:.3f}")
            print(f"      Weather std ratio: {row['std_ratio_weather_mean']:.3f} ± {row['std_ratio_weather_std']:.3f}")
            print()

        print(f"    ✅ Climate-economic correlation analysis completed")
    else:
        print(f"    ❌ No valid results generated")
        raise ValueError("No correlation results were generated")


def analysis_3_placeholder(output_files, analysis_output_dir, config):
    """
    Placeholder for third post-processing analysis.

    Parameters
    ----------
    output_files : dict
        Dictionary containing paths to pipeline output files
    analysis_output_dir : str
        Directory where analysis results should be saved
    config : dict
        Configuration dictionary in main processing format (from obtain_configuration_information)
    """
    print("📊 Analysis 3: [Future analysis placeholder]")
    print("    This analysis slot is ready for future implementation...")
    print(f"    Configuration available: {config['climate_model']['model_name']} model")


def run_all_analyses(output_files, analysis_output_dir, ssp_filter=None):
    """
    Execute all post-processing analyses in sequence.

    First obtains configuration information to enable reuse of main processing functions,
    then runs all specific analyses with access to both output files and configuration.

    Parameters
    ----------
    output_files : dict
        Dictionary containing paths to pipeline output files
    analysis_output_dir : str
        Directory where analysis results should be saved
    ssp_filter : list of str, optional
        List of SSP scenarios to include in analysis. If None, all SSPs are processed.

    Returns
    -------
    dict
        Dictionary containing analysis execution results and timing
    """
    analysis_results = {}
    total_start_time = time.time()

    print(f"\n🔬 Starting post-processing analysis framework...")
    print("=" * 70)

    # First, obtain configuration information (required for all subsequent analyses)
    print(f"\n🔍 Step 1: Obtaining configuration information...")
    config_start = time.time()

    try:
        config = obtain_configuration_information(output_files, analysis_output_dir)
        config_time = time.time() - config_start
        analysis_results['Configuration'] = {
            'status': 'completed',
            'duration': config_time
        }
        print(f"    ✅ Configuration obtained in {config_time:.1f}s")

    except Exception as e:
        config_time = time.time() - config_start
        analysis_results['Configuration'] = {
            'status': 'failed',
            'duration': config_time,
            'error': str(e)
        }
        print(f"    ❌ Configuration failed after {config_time:.1f}s: {e}")
        print(f"    ⚠️  Cannot proceed with analyses without configuration data")

        total_time = time.time() - total_start_time
        analysis_results['total_duration'] = total_time
        return analysis_results

    # Define subsequent analyses (config is now available)
    analyses = [
        ("Temperature-GDP Variability Correlations", analysis_temperature_gdp_correlations),
        ("Analysis 3", analysis_3_placeholder)
    ]

    # Run remaining analyses with both output_files and config available
    print(f"\n🔬 Running {len(analyses)} post-processing analyses...")

    for analysis_name, analysis_func in analyses:
        print(f"\n🔍 Starting {analysis_name}...")
        analysis_start = time.time()

        try:
            # Pass output_files, config, and SSP filter to analyses
            if analysis_func == analysis_temperature_gdp_correlations:
                analysis_func(output_files, analysis_output_dir, config, ssp_filter)
            else:
                analysis_func(output_files, analysis_output_dir, config)
            analysis_time = time.time() - analysis_start
            analysis_results[analysis_name] = {
                'status': 'completed',
                'duration': analysis_time
            }
            print(f"    ✅ {analysis_name} completed in {analysis_time:.1f}s")

        except Exception as e:
            analysis_time = time.time() - analysis_start
            analysis_results[analysis_name] = {
                'status': 'failed',
                'duration': analysis_time,
                'error': str(e)
            }
            print(f"    ❌ {analysis_name} failed after {analysis_time:.1f}s: {e}")

    total_time = time.time() - total_start_time
    analysis_results['total_duration'] = total_time

    print("=" * 70)
    print(f"📋 Post-processing Summary:")

    successful = sum(1 for r in analysis_results.values()
                    if isinstance(r, dict) and r.get('status') == 'completed')
    failed = sum(1 for r in analysis_results.values()
                if isinstance(r, dict) and r.get('status') == 'failed')

    print(f"    ✅ Successful analyses: {successful}")
    print(f"    ❌ Failed analyses: {failed}")
    print(f"    ⏱️  Total processing time: {total_time:.1f}s")

    return analysis_results


def setup_analysis_output_directory(pipeline_output_dir):
    """
    Create a dedicated directory for post-processing analysis results.

    Parameters
    ----------
    pipeline_output_dir : str
        Path to the pipeline output directory

    Returns
    -------
    str
        Path to the created analysis output directory
    """
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

    # Create analysis directory alongside pipeline output
    pipeline_parent = os.path.dirname(os.path.abspath(pipeline_output_dir))
    pipeline_dirname = os.path.basename(os.path.abspath(pipeline_output_dir))

    # Create analysis directory name based on pipeline directory
    analysis_dirname = f"postprocess_{pipeline_dirname}_{timestamp}"
    analysis_output_dir = os.path.join(pipeline_parent, analysis_dirname)

    os.makedirs(analysis_output_dir, exist_ok=True)

    print(f"📁 Analysis output directory: {analysis_output_dir}")
    return analysis_output_dir


def main():
    """Main post-processing entry point."""
    parser = argparse.ArgumentParser(
        description="COIN-SSP Post-Processing Analysis Framework",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python main_postprocess.py data/output/output_integrated_CanESM5_20250919_123456/
  python main_postprocess.py /path/to/pipeline/output/directory/
  python main_postprocess.py /path/to/output/ --ssps ssp126 ssp245
  python main_postprocess.py /path/to/output/ --ssps ssp585

This framework analyzes results from the main.py COIN-SSP processing pipeline.
All analyses are run by default. Use --ssps to limit analysis to specific scenarios.
        """
    )

    parser.add_argument(
        'output_directory',
        help='Path to the output directory created by main.py pipeline'
    )

    parser.add_argument(
        '--ssps',
        nargs='+',
        help='Limit analysis to specific SSP scenarios (e.g., --ssps ssp126 ssp245). If not specified, all SSPs are processed.'
    )

    args = parser.parse_args()

    print("🌍 COIN-SSP Post-Processing Analysis Framework")
    print("=" * 60)
    print(f"Pipeline output directory: {args.output_directory}")
    print(f"Analysis started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    try:
        # Validate input directory and find pipeline output files
        print(f"\n🔍 Validating pipeline output directory...")
        output_files = validate_output_directory(args.output_directory)

        # Setup analysis output directory
        analysis_output_dir = setup_analysis_output_directory(args.output_directory)

        # Run all post-processing analyses
        analysis_results = run_all_analyses(output_files, analysis_output_dir, ssp_filter=args.ssps)

        print(f"\n🎉 Post-processing analysis completed!")
        print(f"📁 Results saved to: {analysis_output_dir}")

        return 0

    except Exception as e:
        print(f"\n❌ Post-processing failed: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())